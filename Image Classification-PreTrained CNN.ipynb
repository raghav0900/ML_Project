{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b232ae79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from IPython.display import display, Image\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a656eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20920d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_ids.txt', 'r') as file:\n",
    "    # Read all lines into a list\n",
    "    lines = file.readlines()\n",
    "    test_ids = []\n",
    "\n",
    "    # Print each line\n",
    "    for line in lines:\n",
    "        test_ids.append(line.strip() + '.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bac303b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>Hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1115290475811037185</td>\n",
       "      <td>@iSpotlight6800 Nigga said Prom Season 😂😂😂😂</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1108180634730745857</td>\n",
       "      <td>idk why mither ben bizzy thought this was a lo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1064098809314230272</td>\n",
       "      <td>😂😂😂😂😂 fs Jackie ya mad cunt</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1113434967588188166</td>\n",
       "      <td>FullyVers nigga</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1114301234767368192</td>\n",
       "      <td>Ayo fr who the fuck is this nigga</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58949</th>\n",
       "      <td>1044597935005216768</td>\n",
       "      <td>When she rolled her sleeves up I knew this int...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58950</th>\n",
       "      <td>1056595445215059969</td>\n",
       "      <td>New Video: Spice – Black Hypocrisy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58951</th>\n",
       "      <td>1062161948748513280</td>\n",
       "      <td>@MSNBC @KellyannePolls Ann shut up you mentall...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58952</th>\n",
       "      <td>1105465552544374786</td>\n",
       "      <td>@quisLaFlare Good luck my nigga 🤘🏾</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58953</th>\n",
       "      <td>1114170734472048640</td>\n",
       "      <td>@svdate @gtconway3d I would just say hes Donny...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>58954 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        id                                         tweet_text  \\\n",
       "0      1115290475811037185       @iSpotlight6800 Nigga said Prom Season 😂😂😂😂    \n",
       "1      1108180634730745857  idk why mither ben bizzy thought this was a lo...   \n",
       "2      1064098809314230272                       😂😂😂😂😂 fs Jackie ya mad cunt    \n",
       "3      1113434967588188166                                   FullyVers nigga    \n",
       "4      1114301234767368192                 Ayo fr who the fuck is this nigga    \n",
       "...                    ...                                                ...   \n",
       "58949  1044597935005216768  When she rolled her sleeves up I knew this int...   \n",
       "58950  1056595445215059969               New Video: Spice – Black Hypocrisy     \n",
       "58951  1062161948748513280  @MSNBC @KellyannePolls Ann shut up you mentall...   \n",
       "58952  1105465552544374786                @quisLaFlare Good luck my nigga 🤘🏾    \n",
       "58953  1114170734472048640  @svdate @gtconway3d I would just say hes Donny...   \n",
       "\n",
       "       Hate  \n",
       "0         0  \n",
       "1         0  \n",
       "2         0  \n",
       "3         0  \n",
       "4         0  \n",
       "...     ...  \n",
       "58949     1  \n",
       "58950     1  \n",
       "58951     1  \n",
       "58952     1  \n",
       "58953     1  \n",
       "\n",
       "[58954 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"with open('train_ids.txt', 'r') as file:\n",
    "    # Read all lines into a list\n",
    "    lines = file.readlines()\n",
    "    train_ids = []\n",
    "\n",
    "    # Print each line\n",
    "    for line in lines:\n",
    "        train_ids.append(line.strip() + '.jpg')\"\"\"\n",
    "\n",
    "df = pd.read_csv(\"train_text.csv\")\n",
    "pd.set_option('display.float_format', '{:.0f}'.format)\n",
    "train_ids = []\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f61a2928",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('val_ids.txt', 'r') as file:\n",
    "    # Read all lines into a list\n",
    "    lines = file.readlines()\n",
    "    val_ids = []\n",
    "\n",
    "    # Print each line\n",
    "    for line in lines:\n",
    "        val_ids.append(line.strip() + '.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29335bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in training dataset:  134823\n",
      "Number of images in validation dataset:  5000\n",
      "Number of images in testing dataset:  10000\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of images in training dataset: \", len(train_ids))\n",
    "print(\"Number of images in validation dataset: \", len(val_ids))\n",
    "print(\"Number of images in testing dataset: \", len(test_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33ff78b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get image files\n",
    "def get_image_files(folder_path):\n",
    "    files = os.listdir(folder_path)\n",
    "    image_files = []\n",
    "    \n",
    "    for file in files:\n",
    "        if file.lower().endswith('.jpg'):\n",
    "            image_files.append(file)\n",
    "    \n",
    "    return image_files\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d0eb839",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '/Users/raghavraahul/Downloads/Machine Learning under a Modern Optimization Lens/ML Project/train'\n",
    "test_path = '/Users/raghavraahul/Downloads/Machine Learning under a Modern Optimization Lens/ML Project/test'\n",
    "val_path = '/Users/raghavraahul/Downloads/Machine Learning under a Modern Optimization Lens/ML Project/valid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c080c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_files(source_path, destination_path, titles):\n",
    "    for title in titles:\n",
    "        source_file = os.path.join(data_directory, title)\n",
    "        destination_file = os.path.join(destination_path, title)\n",
    "        shutil.copy(source_file, destination_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b04cf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_directory = \"/Users/raghavraahul/Downloads/Machine Learning under a Modern Optimization Lens/ML Project/img_resized/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "490e14bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#already done\n",
    "#copy_files(data_directory, train_path, train_ids)\n",
    "#copy_files(data_directory, val_path, val_ids)\n",
    "#copy_files(data_directory, test_path, test_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06f78605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134823"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(get_image_files(train_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7cdba8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Specify the path to your JSON file\n",
    "json_file_path = '/Users/raghavraahul/Downloads/Machine Learning under a Modern Optimization Lens/ML Project/MMHS150K_GT.json'\n",
    "\n",
    "# Open the JSON file for reading\n",
    "with open(json_file_path, 'r') as json_file:\n",
    "    # Load the JSON data\n",
    "    data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8f737fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(data, orient='index')\n",
    "df = df.reset_index()\n",
    "newColumns = {\"index\": \"id\"}\n",
    "df = df.rename(columns=newColumns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5855206",
   "metadata": {},
   "outputs": [],
   "source": [
    "#0 for nothate, #1 for hate\n",
    "def labels_to_binary(labels):\n",
    "    if sum(labels) <= 1:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c289aab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61547"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Hate(1)_vs_NotHate(0)\"] = df[\"labels\"].apply(labels_to_binary)\n",
    "sum(df[\"Hate(1)_vs_NotHate(0)\"]==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e4d239c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nothate_ids = df[df[\"Hate(1)_vs_NotHate(0)\"]==0][\"id\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9961e287",
   "metadata": {},
   "outputs": [],
   "source": [
    "hate_ids = df[df[\"Hate(1)_vs_NotHate(0)\"]==1][\"id\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "35b65bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "nothate_ids = [x + '.jpg' for x in nothate_ids]\n",
    "hate_ids = [x + '.jpg' for x in hate_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac087cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"hate_train_ids = []\n",
    "hate_val_ids = []\n",
    "hate_test_ids = []\n",
    "for id_ in hate_ids:\n",
    "    if id_ in train_ids:\n",
    "        hate_train_ids.append(id_)\n",
    "    elif id_ in test_ids:\n",
    "        hate_test_ids.append(id_)\n",
    "    else:\n",
    "        hate_val_ids.append(id_)\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c42416cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"nothate_train_ids = []\n",
    "nothate_val_ids = []\n",
    "nothate_test_ids = []\n",
    "for id_ in nothate_ids:\n",
    "    if id_ in train_ids:\n",
    "        nothate_train_ids.append(id_)\n",
    "    elif id_ in test_ids:\n",
    "        nothate_test_ids.append(id_)\n",
    "    else:\n",
    "        nothate_val_ids.append(id_)\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "598f1b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "\n",
    "# Set the path to your dataset\n",
    "dataset_path = '/Users/raghavraahul/Downloads/Machine Learning under a Modern Optimization Lens/ML Project'\n",
    "\n",
    "# Define the image size based on the pre-trained model's requirements\n",
    "img_size = (224, 224)\n",
    "\n",
    "# Define the batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Create an ImageDataGenerator for preprocessing images\n",
    "datagen = ImageDataGenerator(preprocessing_function=preprocess_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2db29051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 134823 images belonging to 2 classes.\n",
      "Found 5000 images belonging to 2 classes.\n",
      "Found 10000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Create data generators for training, validation, and test sets\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    os.path.join(dataset_path, 'train'),\n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',  # Assuming you have multiple classes\n",
    "    shuffle=False  # Important: Set shuffle to False for feature extraction\n",
    ")\n",
    "\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "    os.path.join(dataset_path, 'valid'),\n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_generator = datagen.flow_from_directory(\n",
    "    os.path.join(dataset_path, 'test'),\n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04750c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3975/4214 [===========================>..] - ETA: 1:08:57"
     ]
    }
   ],
   "source": [
    "# Load pre-trained VGG16 model\n",
    "base_model = VGG16(weights='/Users/raghavraahul/Downloads/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Extract features from images using the pre-trained model\n",
    "train_features = base_model.predict(train_generator, steps=len(train_generator))\n",
    "validation_features = base_model.predict(validation_generator, steps=len(validation_generator))\n",
    "test_features = base_model.predict(test_generator, steps=len(test_generator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd8c197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the features\n",
    "train_features = np.reshape(train_features, (len(train_generator.classes), -1))\n",
    "validation_features = np.reshape(validation_features, (len(validation_generator.classes), -1))\n",
    "test_features = np.reshape(test_features, (len(test_generator.classes), -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cabdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save for later use to train custom model\n",
    "np.save('train_features.npy', train_features)\n",
    "np.save('validation_features.npy', validation_features)\n",
    "np.save('test_features.npy', test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8b5918",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Load the extracted features\n",
    "train_features = np.load('train_features.npy')\n",
    "validation_features = np.load('validation_features.npy')\n",
    "test_features = np.load('test_features.npy')\n",
    "\n",
    "# Load the corresponding labels for training\n",
    "train_labels = train_generator.classes\n",
    "validation_labels = validation_generator.classes\n",
    "test_labels = test_generator.classes\n",
    "\n",
    "# Define and compile the custom model\n",
    "model = Sequential()\n",
    "model.add(Dense(256, activation='relu', input_dim=train_features.shape[1]))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(lr=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model on the extracted features\n",
    "history = model.fit(train_features, train_labels, \n",
    "                    epochs=10, \n",
    "                    batch_size=32,\n",
    "                    validation_data=(validation_features, validation_labels))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(test_features, test_labels)\n",
    "print(f'Test accuracy: {test_acc * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298b04be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be267580",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7f8a6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cf561c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09c9193",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1234342",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc38506b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d24c6d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531526c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7649d87c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4619967",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d96c14b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa03373",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d2795e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d133851",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c04ead",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2cba5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7e865e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b754af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2950ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574158f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8c6324",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0ce1b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
